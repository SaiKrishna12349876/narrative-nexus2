{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13835985,"sourceType":"datasetVersion","datasetId":8811835}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n!pip install pandas nltk scikit-learn transformers torch textblob -q\n\n# Import libraries\nimport os\nimport pandas as pd\nimport re\nimport nltk\nimport glob\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom textblob import TextBlob\nfrom transformers import pipeline\nfrom google.colab import files\n\n# Download necessary NLTK data to prevent errors\n# This specifically fixes the \"Resource punkt_tab not found\" error\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('punkt_tab', quiet=True) # Added to fix the LookupError\n\nprint(\"‚úÖ All libraries are installed and imported successfully.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Week 1: Data Collection & Input ---\nimport os\nimport glob\n\n# 1. Define the path to your Kaggle input data\n# In Kaggle, data is typically in '../input/dataset-name/'\n# We'll search all subdirectories of ../input/ for .txt files\nkaggle_input_dir = '../input/'\nprint(f\"Searching for .txt files in {kaggle_input_dir}...\")\n\n# 2. Find all .txt files in the input directory and its subdirectories\n# The recursive=True flag searches all subfolders\nfile_paths = glob.glob(os.path.join(kaggle_input_dir, '**', '*.txt'), recursive=True)\n\nif not file_paths:\n    print(\"  - ‚ö†Ô∏è No .txt files found in the '../input/' directory.\")\n    print(\"  - Please use the '+ Add data' button in Kaggle to add your .txt file(s) as a dataset.\")\nelse:\n    for f_path in file_paths:\n        print(f\"  - Found file: '{f_path}'\")\n\n# 3. Define the function to read plain text files (this is unchanged)\ndef read_text_file(filepath):\n    \"\"\"Reads content from a plain .txt file.\"\"\"\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            content = f.read()\n        return content\n    except Exception as e:\n        print(f\"Error reading file {filepath}: {e}\")\n        return None\n\nprint(f\"\\n‚úÖ Week 1 complete. {len(file_paths)} file(s) are ready to be processed.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Week 2: Data Preprocessing ---\n\ndef preprocess_text(text):\n    \"\"\"Cleans, normalizes, and tokenizes text for modeling.\"\"\"\n    text = re.sub(r'[^a-zA-Z\\\\s]', '', text, re.I|re.A)\n    text = text.lower()\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n    return \" \".join(lemmatized_tokens)\n\nprint(\"‚úÖ Week 2 functions defined (preprocess_text).\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Week 3: Topic Modeling (LDA Only) ---\n\ndef get_topics_lda(text, n_topics=5, n_words=7):\n    \"\"\"Extracts topics using LDA and returns them as a string AND a list.\"\"\"\n    report_string = \"--- üî¨ Key Themes (LDA) ---\\n\"\n    documents = [text]\n    vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english')\n    tfidf = vectorizer.fit_transform(documents)\n\n    if tfidf.shape[1] < n_topics:\n        return report_string + \"Warning: Text is too short for meaningful topic modeling.\\n\", []\n\n    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n    lda.fit(tfidf)\n    feature_names = vectorizer.get_feature_names_out()\n\n    topics_list = []  # To store topics for the recommendation engine\n    for topic_idx, topic_words in enumerate(lda.components_):\n        top_words = [feature_names[i] for i in topic_words.argsort()[:-n_words - 1:-1]]\n        report_string += f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\"\n        topics_list.append(top_words) # Add the topic words to our list\n\n    return report_string, topics_list # Return both the report and the list\n\nprint(\"‚úÖ Week 3 functions defined (LDA only).\")\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Week 4: Sentiment Analysis ---\n\ndef get_sentiment(text):\n    \"\"\"Performs sentiment analysis and returns the score and a report string.\"\"\"\n    blob = TextBlob(text)\n    polarity = blob.sentiment.polarity\n\n    if polarity > 0.1: sentiment = 'Positive üòä'\n    elif polarity < -0.1: sentiment = 'Negative üò†'\n    else: sentiment = 'Neutral üòê'\n\n    report_string = f\"--- üìä Sentiment Analysis ---\\n\"\n    report_string += f\"Overall Sentiment: {sentiment}\\n\"\n    report_string += f\"Polarity Score: {polarity:.2f} (Range: -1.0 to +1.0)\\n\"\n\n    return polarity, report_string\n\nprint(\"‚úÖ Week 4 functions defined (get_sentiment).\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Week 5: Summarization & Insights Generation ---\n\ndef generate_summary(text):\n    \"\"\"Generates an abstractive summary and returns it as a string.\"\"\"\n    report_string = \"--- ‚ú® Abstractive Summary ---\\n\"\n    try:\n        # Use device=-1 to force CPU, which is more stable on some Colab instances\n        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=-1)\n        # Truncate text to avoid model length limits\n        summary = summarizer(text[:1024], max_length=150, min_length=30, do_sample=False)\n        report_string += summary[0]['summary_text'] + \"\\n\"\n    except Exception as e:\n        report_string += f\"Could not generate summary: {e}\\n\"\n    return report_string\n\ndef generate_recommendations(polarity, topics):\n    \"\"\"Generates simple actionable insights based on analysis results.\"\"\"\n    report_string = \"--- üí° Actionable Insights & Recommendations ---\\n\"\n\n    if not topics: # Check if topics list is empty\n        report_string += \"Recommendation: No specific topics were identified for recommendations.\\n\"\n        return report_string\n\n    # Get the first 3 words of the most important topic\n    top_topic_words = topics[0][:3]\n\n    if polarity < -0.1:\n        report_string += f\"Recommendation: The overall sentiment is negative. It is advised to investigate feedback related to the key topics identified, such as '{', '.join(top_topic_words)}'.\\\\n\"\n    elif polarity > 0.1:\n        report_string += f\"Recommendation: The positive sentiment is strong. Consider leveraging this momentum by promoting aspects related to topics like '{', '.join(top_topic_words)}'.\\\\n\"\n    else:\n        report_string += \"Recommendation: The sentiment is neutral. Further analysis may be needed to understand specific opinions.\\n\"\n    return report_string\n\nprint(\"‚úÖ Week 5 functions defined (summarization and recommendations).\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Weeks 6-7: Visualization & Reporting ---\n\n# Import the libraries needed for plotting\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud\n\ndef generate_visualizations(text, polarity_score):\n    \"\"\"Generates and displays visualizations (graphs).\"\"\"\n    print(\"\\n\" + \"--- üñºÔ∏è Visualizations ---\")\n\n    # 1. Word Cloud (from your PDF)\n    try:\n        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.show()\n    except ValueError:\n        print(\"Could not generate word cloud (text might be too short).\")\n\n    # 2. Sentiment Distribution Chart (from your PDF)\n    color = 'green' if polarity_score > 0.1 else ('red' if polarity_score < -0.1 else 'grey')\n    fig = go.Figure(go.Bar(x=[polarity_score], y=['Sentiment'], orientation='h', marker=dict(color=color)))\n    fig.update_layout(title='Overall Sentiment Polarity Score', xaxis_title='Score (-1.0 to +1.0)', xaxis=dict(range=[-1, 1]))\n    fig.show()\n\nprint(\"‚úÖ Weeks 6-7 functions defined (generate_visualizations).\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Pipeline Execution (Weeks 1-7) ---\n\nfor path in file_paths:\n    print(f\"\\n\\n{'='*40}\\nüöÄ PROCESSING FILE: {os.path.basename(path)}\\n{'='*40}\")\n\n    if not os.path.exists(path):\n        print(f\"‚ùå Error: File not found at {path}. Skipping.\")\n        continue\n\n    # --- Week 1: Read Data ---\n    clean_text = read_text_file(path)\n    if not clean_text:\n        print(f\"File {path} is empty or could not be read. Skipping.\")\n        continue\n\n    print(\"--- Preview of Cleaned Text (first 300 chars) ---\")\n    print(clean_text[:300] + \"...\")\n\n    # --- Week 2: Preprocess Data ---\n    processed_text_for_model = preprocess_text(clean_text)\n\n    # --- Week 3: Topic Modeling (LDA Only) ---\n    lda_report, topics_list = get_topics_lda(processed_text_for_model)\n\n    # --- Week 4: Sentiment Analysis ---\n    polarity, sentiment_report = get_sentiment(clean_text)\n\n    # --- Week 5: Summarization & Insights ---\n    summary_report = generate_summary(clean_text)\n    recommendation_report = generate_recommendations(polarity, topics_list)\n\n    # --- Print all text reports ---\n    print(\"\\n\" + lda_report)\n    print(sentiment_report)\n    print(summary_report)\n    print(recommendation_report)\n\n    # --- Weeks 6-7: Generate Graphs ---\n    generate_visualizations(processed_text_for_model, polarity)\n\nprint(\"\\n\\n‚úÖ All files processed up to Week 7.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}